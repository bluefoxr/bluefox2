---
author: Will Becker
bg_image: images/featue-bg.jpg
categories:
- Modelling
date: "2020-12-12"
description: And its many applications.
draft: true
image: images/blog/razor.png
tags:
- Modelling
- Statistics
title: Occam's Razor
type: post
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<p>So far in my few posts in this blog I’ve been writing about indicators. Actually I only started working in indicators around 2015. I found that the topic suits me because it is a blend of statistics/data analysis and qualitative work (writing, conceptualising and so on), and I like things that are not too narrowly focused.</p>
<p>Before that (and still now) I was/am a researcher in sensitivity and uncertainty analysis. Uncertainty analysis is understanding how uncertainties in model inputs affect the results, and sensitivity analysis is a more detailed breakdown of which particular model inputs/assumptions are causing the most uncertainty, and by how much.</p>
<p>These topics, combined with some years of experience working in the European Commission, also led me to be generally interested in modelling. I had direct experience building <a href="https://www.sciencedirect.com/science/article/abs/pii/S0888327012000866">engineering</a> and <a href="https://www.sciencedirect.com/science/article/abs/pii/S0021929011002570">biomechanical</a> models in my PhD, and came across many different types of models in the Commission, often used for policy impact assessments.</p>
<p><br/><font size="4"> <strong>What is a model?</strong> </font></p>
<p>First of all, what is a model? Well, as usual there are many ways to divide and classify things, but one distinction is:</p>
<ul>
<li><p><strong>Principle/process-driven models</strong>: these are models that are built based on understanding the physics or processes behind the system. A simple example is <a href="https://en.wikipedia.org/wiki/Hooke%27s_law">Hooke’s Law</a> which describes how a spring extends under a given load. More complex examples are hydrological models and climate models. In all cases, the model is built based on some encoding of the physics or processes driving the system.</p></li>
<li><p><strong>Data-driven models</strong>: in these models, the system is treated much like a black box. Instead, we use a set of measured system inputs and outputs, and try to build a statistical mapping between the two. A linear regression is a simple example, but more complex examples are Gaussian processes, neural networks, deep learning and so on. What they have in common is that the modelling simply tries to replicate the input/output relationship, rather than trying to model any particular physical process.</p></li>
</ul>
<p>No doubt many people would dispute the nuances of those definitions, but I think that the core concept is solid. However, the two categories are not distinct, and in fact will overlap to some extent. At the end of the day, both categories are a system of equations, and both usually have to be calibrated or fitted in some way. This discussion could go on for a while so let’s leave it at that for now.</p>
<p><br/><font size="4"> <strong>Elementary, my dear Occam</strong> </font></p>
<p>Occam’s Razor is one of those heuristics that seems to apply everywhere. According to Wikipedia, it can be defined as:</p>
<blockquote>
<p>the problem-solving principle that “entities should not be multiplied without necessity”, or more simply, the simplest explanation is usually the right one.</p>
</blockquote>
<p>What has this got to do with modelling? Well, everything points to the fact that the simplest model that explains the data/process is the best.</p>
<p>This idea is well-known in statistical modelling (i.e. category 2 of the taxonomy above). You might have heard of the “bias-variance trade off”: this is the idea that there is a balance to be struck between underfitting and overfitting a set of data. Underfitting means that the model is too simple to explain the data/process, whereas overfitting means the opposite: the model gives too much weight to individual observations, rather than focusing on the underlying process.</p>
<p>The general idea is that most of the time, when you are modelling data, you expect a relationship along the lines of:</p>
<p><span class="math display">\[ y = f(x) + \epsilon \]</span>
where <span class="math inline">\(y\)</span> is the variable you want to model, <span class="math inline">\(x\)</span> is the variable, or set of variables that explain <span class="math inline">\(y\)</span>, and <span class="math inline">\(\epsilon\)</span> (which can also be a function of <span class="math inline">\(x\)</span>) is a summary of other “things” that contribute to <span class="math inline">\(y\)</span> but you are not explicitly trying to model. The “things” can errors due to measurement, but they can also be other variables that you are not able to measure, or you prefer not to include in this particular modelling exercise.</p>
<p>The point is that what you want to understand and <em>isolate</em> is <span class="math inline">\(f(x)\)</span>, <em>not</em> <span class="math inline">\(\epsilon\)</span>. If your model starts to include elements of <span class="math inline">\(\epsilon\)</span>, then you have a mix of the two quantities, and this makes it difficult/impossible to (a) understand the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(f(x)\)</span>, and (b) make predictions of <span class="math inline">\(y\)</span> at unobserved values of <span class="math inline">\(x\)</span>. Occam’s Razor can be statistically proven - a nice example is in David Mackay’s excellent <a href="http://www.inference.org.uk/itprnn/book.pdf">Information Theory, Inference, and Learning Algorithms</a> book - see Chapter 28.</p>
<p>Anyway, let’s visualise this idea. Here’s some data: <span class="math inline">\(x\)</span> is random numbers, <span class="math inline">\(y = 2x + \epsilon\)</span>, where $ $ is normally-distributed noise.</p>
<pre class="r"><code>library(plotly)
x &lt;- runif(20) %&gt;% sort() # random numbers (sorted to avoid problems in plotting)
y &lt;- 2*x + rnorm(20)*0.5 # create y and add noise
df &lt;- data.frame(x, y)
fig &lt;- plot_ly(data = df, x = ~x, y = ~y) # plot
fig</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"visdat":{"3e58473b34cf":["function () ","plotlyVisDat"]},"cur_data":"3e58473b34cf","attrs":{"3e58473b34cf":{"x":{},"y":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20]}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"title":"x"},"yaxis":{"domain":[0,1],"automargin":true,"title":"y"},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0.05679847067222,0.0619519385509193,0.102853653719649,0.189109592698514,0.311039448948577,0.585398110328242,0.689849885879084,0.722107801353559,0.726398337166756,0.73948583053425,0.744786718627438,0.787648085970432,0.810337271308526,0.854878849117085,0.86225473578088,0.927676803665236,0.932944029569626,0.937172397971153,0.975128312129527,0.995259639807045],"y":[0.302320030847748,-0.152718639561883,-0.203310204424327,0.504007552325007,0.81931966595389,1.83161820824218,0.901910866459467,1.32464037747039,1.29969795515333,0.922640607187968,1.48273982781417,1.53911898668598,2.43906415188829,2.0618841126846,2.09088852561599,1.12737212955199,1.87727878612941,1.81138949810807,2.01797737134037,1.98629968192484],"type":"scatter","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Now let’s consider two possibilities. We want to know how <span class="math inline">\(y\)</span> is related to <span class="math inline">\(x\)</span>. In the first case, we fit a straight line through the data (this is cheating because it is the real relationship).</p>
<pre class="r"><code>f &lt;- lm(y ~ x, data=df) # fit linear regression
df &lt;- cbind(df,ylin=f$fitted.values) # add to data frame

# plot
fig &lt;- plot_ly(data = df, x = ~x)
fig &lt;- fig %&gt;% add_trace(y = ~y, name = &#39;Data&#39;,mode = &#39;markers&#39;)
fig &lt;- fig %&gt;% add_trace(y = ~ylin, name = &#39;Linear fit&#39;,mode = &#39;lines&#39;)
fig</code></pre>
<div id="htmlwidget-2" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"visdat":{"3e586f7f5ad8":["function () ","plotlyVisDat"]},"cur_data":"3e586f7f5ad8","attrs":{"3e586f7f5ad8":{"x":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"y":{},"name":"Data","mode":"markers","inherit":true},"3e586f7f5ad8.1":{"x":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"y":{},"name":"Linear fit","mode":"lines","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"title":"x"},"yaxis":{"domain":[0,1],"automargin":true,"title":"y"},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0.05679847067222,0.0619519385509193,0.102853653719649,0.189109592698514,0.311039448948577,0.585398110328242,0.689849885879084,0.722107801353559,0.726398337166756,0.73948583053425,0.744786718627438,0.787648085970432,0.810337271308526,0.854878849117085,0.86225473578088,0.927676803665236,0.932944029569626,0.937172397971153,0.975128312129527,0.995259639807045],"y":[0.302320030847748,-0.152718639561883,-0.203310204424327,0.504007552325007,0.81931966595389,1.83161820824218,0.901910866459467,1.32464037747039,1.29969795515333,0.922640607187968,1.48273982781417,1.53911898668598,2.43906415188829,2.0618841126846,2.09088852561599,1.12737212955199,1.87727878612941,1.81138949810807,2.01797737134037,1.98629968192484],"name":"Data","mode":"markers","type":"scatter","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[0.05679847067222,0.0619519385509193,0.102853653719649,0.189109592698514,0.311039448948577,0.585398110328242,0.689849885879084,0.722107801353559,0.726398337166756,0.73948583053425,0.744786718627438,0.787648085970432,0.810337271308526,0.854878849117085,0.86225473578088,0.927676803665236,0.932944029569626,0.937172397971153,0.975128312129527,0.995259639807045],"y":[0.0820418068693642,0.0926043449456562,0.176436423465259,0.35322643384572,0.603133638002622,1.16545863460563,1.37954279741807,1.44565856033361,1.45445243447066,1.48127653627379,1.49214122654456,1.57998980447663,1.6264935160927,1.7177858502364,1.73290345367674,1.86699240102703,1.87778809733893,1.88645455326588,1.96424892357241,2.00551005493581],"name":"Linear fit","mode":"lines","type":"scatter","marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Looks good, and it is close to the actual relationship. But in practice, if all we have is a set of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values, we wouldn’t know whether the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is linear, or something more complicated. We might consider fitting a higher-order polynomial, for example. This might seem like a good idea, because after all, a linear model is a special case of a higher order polynomial, so we can’t lose anything by making it more complicated - it will still provide the best fit, right?</p>
<pre class="r"><code>f2 &lt;- lm(y ~ poly(x,5), data=df) # fit 5th-order polynomial
df &lt;- cbind(df, ypoly=f2$fitted.values, ytrue = 2*x) # add to data frame

# plot
fig &lt;- plot_ly(data = df, x = ~x)
fig &lt;- fig %&gt;% add_trace(y = ~y, name = &#39;Data&#39;,mode = &#39;markers&#39;)
fig &lt;- fig %&gt;% add_trace(y = ~ytrue, name = &#39;True f(x)&#39;,mode = &#39;lines&#39;)
fig &lt;- fig %&gt;% add_trace(y = ~ylin, name = &#39;Linear fit&#39;,mode = &#39;lines&#39;)
fig &lt;- fig %&gt;% add_trace(y = ~ypoly, name = &#39;5th-order poly fit&#39;,mode = &#39;lines&#39;)
fig</code></pre>
<div id="htmlwidget-3" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-3">{"x":{"visdat":{"3e5817885b2a":["function () ","plotlyVisDat"]},"cur_data":"3e5817885b2a","attrs":{"3e5817885b2a":{"x":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"y":{},"name":"Data","mode":"markers","inherit":true},"3e5817885b2a.1":{"x":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"y":{},"name":"True f(x)","mode":"lines","inherit":true},"3e5817885b2a.2":{"x":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"y":{},"name":"Linear fit","mode":"lines","inherit":true},"3e5817885b2a.3":{"x":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"y":{},"name":"5th-order poly fit","mode":"lines","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"title":"x"},"yaxis":{"domain":[0,1],"automargin":true,"title":"y"},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0.05679847067222,0.0619519385509193,0.102853653719649,0.189109592698514,0.311039448948577,0.585398110328242,0.689849885879084,0.722107801353559,0.726398337166756,0.73948583053425,0.744786718627438,0.787648085970432,0.810337271308526,0.854878849117085,0.86225473578088,0.927676803665236,0.932944029569626,0.937172397971153,0.975128312129527,0.995259639807045],"y":[0.302320030847748,-0.152718639561883,-0.203310204424327,0.504007552325007,0.81931966595389,1.83161820824218,0.901910866459467,1.32464037747039,1.29969795515333,0.922640607187968,1.48273982781417,1.53911898668598,2.43906415188829,2.0618841126846,2.09088852561599,1.12737212955199,1.87727878612941,1.81138949810807,2.01797737134037,1.98629968192484],"name":"Data","mode":"markers","type":"scatter","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[0.05679847067222,0.0619519385509193,0.102853653719649,0.189109592698514,0.311039448948577,0.585398110328242,0.689849885879084,0.722107801353559,0.726398337166756,0.73948583053425,0.744786718627438,0.787648085970432,0.810337271308526,0.854878849117085,0.86225473578088,0.927676803665236,0.932944029569626,0.937172397971153,0.975128312129527,0.995259639807045],"y":[0.11359694134444,0.123903877101839,0.205707307439297,0.378219185397029,0.622078897897154,1.17079622065648,1.37969977175817,1.44421560270712,1.45279667433351,1.4789716610685,1.48957343725488,1.57529617194086,1.62067454261705,1.70975769823417,1.72450947156176,1.85535360733047,1.86588805913925,1.87434479594231,1.95025662425905,1.99051927961409],"name":"True f(x)","mode":"lines","type":"scatter","marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[0.05679847067222,0.0619519385509193,0.102853653719649,0.189109592698514,0.311039448948577,0.585398110328242,0.689849885879084,0.722107801353559,0.726398337166756,0.73948583053425,0.744786718627438,0.787648085970432,0.810337271308526,0.854878849117085,0.86225473578088,0.927676803665236,0.932944029569626,0.937172397971153,0.975128312129527,0.995259639807045],"y":[0.0820418068693642,0.0926043449456562,0.176436423465259,0.35322643384572,0.603133638002622,1.16545863460563,1.37954279741807,1.44565856033361,1.45445243447066,1.48127653627379,1.49214122654456,1.57998980447663,1.6264935160927,1.7177858502364,1.73290345367674,1.86699240102703,1.87778809733893,1.88645455326588,1.96424892357241,2.00551005493581],"name":"Linear fit","mode":"lines","type":"scatter","marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[0.05679847067222,0.0619519385509193,0.102853653719649,0.189109592698514,0.311039448948577,0.585398110328242,0.689849885879084,0.722107801353559,0.726398337166756,0.73948583053425,0.744786718627438,0.787648085970432,0.810337271308526,0.854878849117085,0.86225473578088,0.927676803665236,0.932944029569626,0.937172397971153,0.975128312129527,0.995259639807045],"y":[0.0629905126170824,0.0362116094081038,-0.0415136885241558,0.285196680256059,0.999258695359424,1.35773405440585,1.36348726862787,1.40529607193133,1.41256599951685,1.43720073845247,1.44821480617952,1.55714107253534,1.62616666205658,1.76944335424757,1.79228380053851,1.93201757008521,1.93490576983073,1.93590964673195,1.87998884599487,1.78964002114629],"name":"5th-order poly fit","mode":"lines","type":"scatter","marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"line":{"color":"rgba(214,39,40,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>As you can see, this was <em>not</em> a good idea. The higher-order model overfits the data. Because it has more flexibility in its shape, it is able to get closer to some of the outlying points, but by doing so it is confusing the noise with the underlying <em>linear</em> relationship, <span class="math inline">\(f(x)\)</span>.</p>
<p><br/><font size="4"> <strong>Occam in the Real World</strong> </font></p>
<p>OK, so outside of regression, what’s the implication? Well, as we have discussed here, any model, be it a large physical model or a simple regression, is a mapping of inputs to outputs. It aims to emulate a system. If your model is calibrated to observations (if it’s not, that’s a problem in itself), then you are in exactly the same situation as the example above. In short, if you build a hugely complicated model you run the risk of overfitting it to observation data, which actually results in poorer prediction capability than a simple model.</p>
<p>It’s important to point out amid all this complex-model-bashing that this is not about ignoring complexity, but striking a balance. There is as much risk in underfitting as overfitting. Clearly, you can’t model a complex nonlinear system with a linear model either. So, what to do?</p>
<p>Luckily these problems have been studied by very clever people for many years, in a field known as <a href="https://en.wikipedia.org/wiki/Model_selection#Criteria">model selection</a>, and there are a range of tools such as information criterion, Bayes factors, cross-validation and more.</p>
<p>A problem here is that while it is easy in statistical modelling to build many different alternative models and compare them, in process-driven models this is much more difficult. While alternative models can be compared, it is probably worth keeping in mind that adding more and more complexity to a model, and throwing more computing power at it, does not necessarily reduce the uncertainty. To make that point, consider how the uncertainty in climate sensitivity has changed over the last 30 years:</p>
<ul>
<li>1979: 1.5-4.5C [National Academy of Sciences]</li>
<li>1990: 1.5-4.0C [IPCC first report]</li>
<li>1996: 1.5-4.0C [IPCC second report]</li>
<li>2007: 2.0-4.5C [IPCC fourth report]</li>
<li>2014: 1.5-4.5C [IPCC fifth report]</li>
</ul>
<p>Since 1979, computing power has increased at least a hundredfold. But the estimated uncertainty has actually remained the same.</p>
<p><br/><font size="4"> <strong>Finally</strong> </font></p>
<p><img src="/english/blog/2020-01-01-OccamsRazor_files/complex_vs_simple_models.png" alt="Model tradeoff" />
This is a slightly whimsical slide that I’ve used in the past to elaborate on this problem. A simple model is further away from reality in terms of “structural uncertainty”, i.e. the uncertainty due to the fact that the model simplifies the complexities of the real system. But there is actually <em>less</em> uncertainty in its parameters because there are less parameters. In the linear model, there are only two parameters.</p>
<p>Whereas, if we go to a more complex model, it becomes closer to “reality” in that it inludes more of the complexities of the real system, but now we have more parameters to fit, so there can actually be <em>more</em> parametric uncertainty! In the fifth-order polynomial used above, there are 6 parameters, and it was clear that it actually caused more trouble than it was worth.</p>
<p>Ok that’s it for now folks. This is a long topic and I have other work to get on with. I intend to continue this series on the ins and outs of modelling and uncertainty.</p>
<p>Image by <a href="https://pixabay.com/users/clker-free-vector-images-3736/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=33226">Clker-Free-Vector-Images</a> from <a href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=33226">Pixabay</a></p>
